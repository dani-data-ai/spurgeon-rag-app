# Spurgeon RAG App - Architecture & Overview

## ğŸ¯ What It Does

A **Retrieval-Augmented Generation (RAG)** web application that allows users to ask questions about Charles Spurgeon's sermons and receive AI-powered answers based on actual sermon content.

### User Flow:
1. User asks: *"What does Spurgeon say about faith?"*
2. System searches sermon database for relevant passages about faith
3. Retrieves top 4 most relevant sermon excerpts
4. Sends excerpts + question to local LLM (Llama 3.2 8B)
5. LLM generates answer based only on retrieved context
6. User sees answer + can view source sermon excerpts

---

## ğŸ—ï¸ System Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     USER INTERFACE                          â”‚
â”‚                   (Streamlit Web App)                       â”‚
â”‚  - Mobile-first responsive design                           â”‚
â”‚  - Chat interface with source citations                     â”‚
â”‚  - Real-time configuration                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    RAG PIPELINE FLOW                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚  1. QUERY INPUT                                             â”‚
â”‚     â””â”€ User question via chat input                         â”‚
â”‚                                                             â”‚
â”‚  2. EMBEDDING & RETRIEVAL                                   â”‚
â”‚     â”œâ”€ Convert question to vector (all-MiniLM-L6-v2)        â”‚
â”‚     â”œâ”€ Semantic search in ChromaDB                          â”‚
â”‚     â””â”€ Retrieve top-k relevant chunks (default: 4)          â”‚
â”‚                                                             â”‚
â”‚  3. CONTEXT ASSEMBLY                                        â”‚
â”‚     â””â”€ Combine retrieved chunks into prompt context         â”‚
â”‚                                                             â”‚
â”‚  4. LLM GENERATION                                          â”‚
â”‚     â”œâ”€ Send context + question to LM Studio API            â”‚
â”‚     â”œâ”€ Llama 3.2 8B generates answer                        â”‚
â”‚     â””â”€ Answer constrained to provided context               â”‚
â”‚                                                             â”‚
â”‚  5. RESPONSE RENDERING                                      â”‚
â”‚     â”œâ”€ Display answer in chat                               â”‚
â”‚     â””â”€ Show source citations with expandable excerpts       â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“ Project Structure

```
spurgeon-rag-app/
â”‚
â”œâ”€â”€ app.py                          # Main Streamlit web application
â”‚   â”œâ”€â”€ Page configuration & mobile-first CSS
â”‚   â”œâ”€â”€ Session state management
â”‚   â”œâ”€â”€ Sidebar settings (LM Studio, RAG params)
â”‚   â”œâ”€â”€ Vector DB loading (@st.cache_resource)
â”‚   â”œâ”€â”€ Query function (retrieval + generation)
â”‚   â””â”€â”€ Chat UI with source display
â”‚
â”œâ”€â”€ setup_rag.py                    # One-time vector DB setup script
â”‚   â”œâ”€â”€ PDF loading (from C:\Users\danieo\Downloads\sp-library)
â”‚   â”œâ”€â”€ Text chunking (1000 chars, 200 overlap)
â”‚   â”œâ”€â”€ Embedding generation (all-MiniLM-L6-v2)
â”‚   â””â”€â”€ ChromaDB persistence
â”‚
â”œâ”€â”€ query_spurgeon.py               # CLI alternative (legacy)
â”‚   â””â”€â”€ Command-line RAG interface
â”‚
â”œâ”€â”€ requirements.txt                # Python dependencies
â”‚   â”œâ”€â”€ streamlit (web framework)
â”‚   â”œâ”€â”€ langchain (RAG framework)
â”‚   â”œâ”€â”€ chromadb (vector database)
â”‚   â”œâ”€â”€ sentence-transformers (embeddings)
â”‚   â””â”€â”€ requests (HTTP client)
â”‚
â”œâ”€â”€ vector_db/                      # Persisted vector database
â”‚   â”œâ”€â”€ chroma.sqlite3              # SQLite metadata store (56.8 MB)
â”‚   â””â”€â”€ 5836d13c-.../               # Vector embeddings
â”‚
â”œâ”€â”€ README.md                       # User setup guide
â”œâ”€â”€ MOBILE_OPTIMIZATION.md          # Mobile-first improvements
â””â”€â”€ ARCHITECTURE.md                 # This file
```

---

## ğŸ”§ Core Components

### 1. **Embedding Model** (all-MiniLM-L6-v2)
- **Purpose**: Convert text to 384-dimensional vectors
- **Where Used**:
  - Setup: Embed all sermon chunks â†’ vector DB
  - Runtime: Embed user questions â†’ semantic search
- **Performance**: CPU-optimized, fast inference
- **Library**: HuggingFace Sentence Transformers

### 2. **Vector Database** (ChromaDB)
- **Purpose**: Store & search sermon embeddings
- **Size**: ~56.8 MB (sermon collection)
- **Search Method**: Cosine similarity
- **Persistence**: SQLite backend
- **Location**: `./vector_db/`

### 3. **LLM Inference** (LM Studio + Llama 3.2 8B)
- **Deployment**: Local server (http://10.160.212.212:1234)
- **Model**: llama3.2-8b-ins-grpo
- **API**: OpenAI-compatible endpoints
- **Parameters**:
  - Temperature: 0.7 (configurable)
  - Max tokens: 500 (configurable)
  - Timeout: 60s

### 4. **Web Framework** (Streamlit)
- **UI Type**: Single-page chat application
- **State Management**: st.session_state
- **Caching**: @st.cache_resource for embeddings
- **Mobile**: Responsive CSS, collapsed sidebar

### 5. **RAG Framework** (LangChain)
- **Document Loaders**: PyPDFLoader
- **Text Splitters**: RecursiveCharacterTextSplitter
- **Vector Stores**: Chroma integration
- **Embeddings**: HuggingFaceEmbeddings wrapper

---

## ğŸ”„ Data Flow

### Setup Phase (One-time)
```
PDFs â†’ Load â†’ Split into chunks â†’ Embed â†’ Store in ChromaDB
       (PyPDF)  (1000/200 chars)   (384-d)   (56.8 MB)
```

### Query Phase (Runtime)
```
Question â†’ Embed â†’ Similarity Search â†’ Top-k chunks â†’ Prompt â†’ LLM â†’ Answer
          (384-d)   (ChromaDB)          (k=4)         Assembly  (Llama)
```

---

## ğŸŒ Network Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     HTTP      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Browser    â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚  Streamlit App  â”‚
â”‚  (User)      â”‚   localhost    â”‚  (port 8501)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     :8501      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                          â”‚
                                          â”‚ Reads
                                          â–¼
                                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                 â”‚   ChromaDB      â”‚
                                 â”‚  (vector_db/)   â”‚
                                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                          â”‚
                                          â”‚ HTTP POST
                                          â–¼
                                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                 â”‚   LM Studio     â”‚
                                 â”‚  10.160.212.212 â”‚
                                 â”‚    :1234        â”‚
                                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ¨ Mobile-First Design

### Responsive Breakpoints:
- **Mobile**: < 768px (1rem padding, collapsed sidebar)
- **Tablet**: 768px - 1023px (2rem padding, 900px max-width)
- **Desktop**: â‰¥ 1024px (3rem padding, 1100px max-width)

### Touch Optimization:
- 44px minimum touch targets (Apple/Android guidelines)
- 16px font size (prevents iOS zoom)
- Full-width buttons on mobile
- Collapsible sidebar by default

---

## ğŸ“Š Performance Characteristics

### Latency Breakdown:
1. **Embedding**: ~50-200ms (CPU, question â†’ vector)
2. **Retrieval**: ~10-50ms (ChromaDB similarity search)
3. **LLM Inference**: ~2-10s (depends on token count, network)
4. **Total**: ~2.5-11s per query

### Resource Usage:
- **Memory**: ~1-2 GB (Streamlit + embeddings model)
- **Disk**: 56.8 MB (vector DB)
- **Network**: ~1-10 KB/query (LM Studio API)

### Bottlenecks:
1. **LLM inference time** (largest contributor)
2. **Cold start**: Embedding model load (~3-5s first query)
3. **Network latency**: To LM Studio server

---

## ğŸ” Security Considerations

### Current State (NOT Production Ready):
âŒ Hardcoded IP addresses
âŒ No authentication/authorization
âŒ No input validation/sanitization
âŒ No rate limiting
âŒ No HTTPS/TLS encryption
âŒ Exposed internal network endpoints

### Required for Production:
âœ… Environment variables for config
âœ… API key authentication
âœ… Input validation & sanitization
âœ… Rate limiting (per-user/IP)
âœ… HTTPS with valid certificates
âœ… Network segmentation
âœ… Logging & monitoring

---

## ğŸš€ Scalability Path

### Current: Single-User Development
- Local embedding model
- Single ChromaDB instance
- Direct LM Studio connection

### Next: Multi-User Deployment
- Shared ChromaDB server
- Load-balanced LM Studio instances
- Caching layer (Redis)
- Async request handling

### Future: Production Scale
- Managed vector DB (Pinecone, Weaviate)
- Cloud LLM API (Anthropic, OpenAI)
- CDN for static assets
- Kubernetes orchestration

---

## ğŸ› ï¸ Technology Stack

| Layer | Technology | Purpose |
|-------|-----------|---------|
| **Frontend** | Streamlit | Web UI framework |
| **Backend** | Python 3.8+ | Application logic |
| **Embeddings** | all-MiniLM-L6-v2 | Text â†’ vector conversion |
| **Vector DB** | ChromaDB | Similarity search |
| **LLM** | Llama 3.2 8B | Answer generation |
| **LLM Server** | LM Studio | Local inference |
| **Framework** | LangChain | RAG orchestration |
| **PDF Processing** | PyPDF | Document loading |

---

## ğŸ“ˆ Key Metrics

- **Vector DB Size**: 56.8 MB
- **Embedding Dimensions**: 384
- **Chunk Size**: 1000 characters
- **Chunk Overlap**: 200 characters
- **Default Context Chunks**: 4
- **LLM Temperature**: 0.7
- **Max Tokens**: 500
- **Request Timeout**: 60 seconds

---

## ğŸ’¡ How RAG Works (Simple Explanation)

**Traditional ChatGPT:**
- User asks question â†’ LLM answers from training data
- Problem: Can hallucinate, no source citations

**This RAG System:**
1. User asks question
2. Find relevant sermon excerpts (semantic search)
3. Give LLM only those excerpts + question
4. LLM answers based only on provided context
5. User sees answer + exact sources

**Benefits:**
âœ… Grounded in actual sermon text
âœ… Source citations for verification
âœ… No hallucinations about Spurgeon
âœ… Works with custom/private documents
âœ… Lower cost than fine-tuning

---

## ğŸ¯ Use Cases

1. **Theological Research**: Find Spurgeon's views on specific topics
2. **Sermon Preparation**: Quote accurate sources in sermons
3. **Bible Study**: Explore Spurgeon's commentary on verses
4. **Historical Analysis**: Understand 19th-century Reformed theology
5. **Educational**: Learn from Spurgeon's preaching style

---

## ğŸ”® Future Enhancements

- [ ] Multi-document support (add more authors)
- [ ] Conversation memory (follow-up questions)
- [ ] Advanced filters (date, sermon series, Bible passage)
- [ ] Export answers as PDF/Markdown
- [ ] Audio sermon integration
- [ ] Multilingual support
- [ ] Fine-tuned embeddings for theology
- [ ] Hybrid search (keyword + semantic)
